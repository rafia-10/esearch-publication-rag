{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_Be-a5qf3fgL"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "import chromadb\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "import os\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 : Setting up the knowledge base"
      ],
      "metadata": {
        "id": "QnUwUWEV7JLE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75378abd"
      },
      "source": [
        "# initialise chromaDB\n",
        "client= chromadb.PersistentClient(path='./research_db')\n",
        "collection = client.get_or_create_collection(name='ml_publication',\n",
        "                                             metadata={'hnsw:space':'cosine'})"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2: Loading the Publications ðŸ“–"
      ],
      "metadata": {
        "id": "9FK-AjhS9Azs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_research_publications(documents_path):\n",
        "    \"\"\"Load research publications from .txt files and return as list of strings\"\"\"\n",
        "\n",
        "\n",
        "    documents = []\n",
        "\n",
        "    # Load each .txt file in the documents folder\n",
        "    for file in os.listdir(documents_path):\n",
        "        if file.endswith(\".txt\"):\n",
        "            file_path = os.path.join(documents_path, file)\n",
        "            try:\n",
        "                loader = TextLoader(file_path)\n",
        "                loaded_docs = loader.load()\n",
        "                documents.extend(loaded_docs)\n",
        "                print(f\"Successfully loaded: {file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {file}: {str(e)}\")\n",
        "\n",
        "    print(f\"\\nTotal documents loaded: {len(documents)}\")\n",
        "\n",
        "    # Extract content as strings and return\n",
        "    publications = []\n",
        "    for doc in documents:\n",
        "        publications.append(doc.page_content)\n",
        "\n",
        "    return publications"
      ],
      "metadata": {
        "id": "xIrViwOsA_qo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 3: Chunking publications"
      ],
      "metadata": {
        "id": "cCrRf_3ZBaRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_research_paper(paper_content, title):\n",
        "    \"\"\"Break a research paper into searchable chunks\"\"\"\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,          # ~200 words per chunk\n",
        "        chunk_overlap=200,        # Overlap to preserve context\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "    )\n",
        "\n",
        "    chunks = text_splitter.split_text(paper_content)\n",
        "\n",
        "    # Add metadata to each chunk\n",
        "    chunk_data = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk_data.append({\n",
        "            \"content\": chunk,\n",
        "            \"title\": title,\n",
        "            \"chunk_id\": f\"{title}_{i}\",\n",
        "        })\n",
        "\n",
        "    return chunk_data"
      ],
      "metadata": {
        "id": "hOmnLmrt9Kim"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4 : Creating Embeddings"
      ],
      "metadata": {
        "id": "e6-ChLaHBhLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def embed_documents(documents: list[str]) -> list[list[float]]:\n",
        "    \"\"\"\n",
        "    Embed documents using a model.\n",
        "    \"\"\"\n",
        "    device = (\n",
        "        \"cuda\"\n",
        "        if torch.cuda.is_available()\n",
        "        else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "    )\n",
        "    model = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        model_kwargs={\"device\": device},\n",
        "    )\n",
        "\n",
        "    embeddings = model.embed_documents(documents)\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "BVdFIb5N9KQ4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "keM9nj-DCBnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Storing in Vector Database ðŸ’¾"
      ],
      "metadata": {
        "id": "DFIr1yWjfqPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_publications(collection:chromadb.Collection, publications : list[str]):\n",
        "  \"\"\"\n",
        "  Insert documents into a ChromaDB collection.\n",
        "\n",
        "    Args:\n",
        "        collection (chromadb.Collection): The collection to insert documents into\n",
        "        publications (list[str]): The documents to insert\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "  \"\"\"\n",
        "  next_id = collection.count()\n",
        "  for publication in publications:\n",
        "    chunked_publication = chunk_research_paper(publication)\n",
        "    embeddings = embed_documents(chunked_publication)\n",
        "    ids = list(range(next_id, next_id + len(chunked_publication)))\n",
        "    ids = [f\"document_{id}\" for id in ids]\n",
        "    collection.add(\n",
        "        embeddings=embeddings,\n",
        "        ids=ids,\n",
        "        documents=chunked_publication\n",
        "    )"
      ],
      "metadata": {
        "id": "cn3cecLY6lkv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 6: Intelligent Retrieval ðŸŽ¯"
      ],
      "metadata": {
        "id": "Dsi8IHSdisHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_research_db(query, collection, embedding_model, top_k=5):\n",
        "   # Convert question to vector using the embedding model\n",
        "    query_vector = embedding_model.embed_query(query)\n",
        "\n",
        "    # Search for similar content\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_vector],\n",
        "        n_results=top_k,\n",
        "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
        "    )\n",
        "\n",
        "    # Format results\n",
        "    relevant_chunks = []\n",
        "    for i, doc in enumerate(results[\"documents\"][0]):\n",
        "        relevant_chunks.append({\n",
        "            \"content\": doc,\n",
        "            \"title\": results[\"metadatas\"][0][i][\"title\"],\n",
        "            \"similarity\": 1 - results[\"distances\"][0][i]  # Convert distance to similarity\n",
        "        })\n",
        "\n",
        "    return relevant_chunks"
      ],
      "metadata": {
        "id": "OuckvuOfirnV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Generating Research-Backed Answers ðŸ¤–"
      ],
      "metadata": {
        "id": "orlCZkbJkwLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "import os\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "import torch\n",
        "\n",
        "def answer_research_question(query, collection, embedding_model, llm):\n",
        "    \"\"\"Generate an answer based on retrieved research\"\"\"\n",
        "\n",
        "    # Get relevant research chunks\n",
        "    relevant_chunks = search_research_db(query, collection, embedding_model, top_k=3)\n",
        "\n",
        "    # Build context from research\n",
        "    context = \"\\n\\n\".join([\n",
        "        f\"From {chunk['title']}:\\n{chunk['content']}\"\n",
        "        for chunk in relevant_chunks\n",
        "    ])\n",
        "\n",
        "    # Create research-focused prompt\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "        template=\"\"\"\n",
        "Based on the following research findings, answer the researcher's question:\n",
        "\n",
        "Research Context:\n",
        "{context}\n",
        "\n",
        "Researcher's Question: {question}\n",
        "\n",
        "Answer: Provide a comprehensive answer based on the research findings above.\n",
        "\"\"\"\n",
        "    )\n",
        "\n",
        "    # Generate answer\n",
        "    prompt = prompt_template.format(context=context, question=query)\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content, relevant_chunks\n",
        "\n",
        "# Retrieve the API key from Colab's Secrets Manager\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "# Set the environment variable\n",
        "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatGroq(model=\"llama3-8b-8192\")\n",
        "\n",
        "# Initialize embedding model\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        ")\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={\"device\": device},\n",
        ")\n",
        "\n",
        "# Load publications (assuming this function is defined elsewhere and works)\n",
        "publications = load_research_publications('./research_db') # This line might need adjustment based on your actual data loading\n",
        "\n",
        "# Call the answer_research_question function\n",
        "#Note: You need to make sure 'collection' is initialized and populated with documents\n",
        "#and 'publications' is loaded before running this.\n",
        "answer, sources = answer_research_question(\n",
        "    \"What are effective techniques for handling class imbalance?\",\n",
        "    collection,\n",
        "    embedding_model, # Pass the embedding model object\n",
        "    llm\n",
        ")\n",
        "\n",
        "print(\"AI Answer:\", answer)\n",
        "print(\"\\nBased on sources:\")\n",
        "for source in sources:\n",
        "    print(f\"- {source['title']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8iAqnfGj36R",
        "outputId": "fe78b7a8-cb13-41e6-fc0b-bcd5e6e27ac9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total documents loaded: 0\n",
            "AI Answer: I apologize, but there is no research finding provided above. Therefore, I cannot answer the researcher's question.\n",
            "\n",
            "Based on sources:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xy36DqyHkll7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}